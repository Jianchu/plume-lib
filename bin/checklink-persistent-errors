#! /usr/bin/env python

# This script allows you to ignore transient errors in checklink output.
# Run it like this:
#   checklink-persistent-errors BASENAME N
# BASENAME is a prefix for files containing the output of the checklink program.
# N is an integer; this program will read that many of the most recent files.

# The checklink program checks for broken links.  It is a hassle when
# checklink reports a broken link that is really due to network problems or
# server downtime.  This program takes the output of multiple runs of
# checklink, and it only reports the errors that appear in all of them.

# For example, you might put in your crontab file:
#   pl=/homes/gws/mernst/bin/src/plume-lib
#   00 20 * * *	$pl/bin/checklink -q -r `grep -v '^#' $pl/bin/checklink-args.txt` http://www.cs.washington.edu/education/courses/cse140/13wi/ > /scratch/mernst/checklink-output/140.txt-`date +\%Y\%m\%d` 2>&1
#   59 20 * * *	$pl/bin/checklink-persistent-errors /scratch/mernst/checklink-output/140.txt 2
# where the first job runs checklink every day, saving the output to a file,
# and the second job reports all errors that are in the two most recent files.

# This script could optionally delete older output files.
# But, they are small and there is no harm in keeping them around indefinitely.

import glob
import os
import pprint
pp = pprint.PrettyPrinter()
import re
import sys

debug=False
# For debugging
# debug=True

def main():

	if len(sys.argv) != 3:
		print ("%s requires exactly 2 arguments, got %d" % (sys.argv[0], len(sys.argv)-1))
	        exit(1)
	basename = sys.argv[1]
	instances = int(sys.argv[2])
	
	files = glob.glob(basename + "*")
        if len(files) < instances:
                exit(0)

	# Sort by file time
	files.sort(key=lambda x: os.stat(x).st_mtime)
	
        # results is a list of maps
        results = []
	for i in range(0, instances):
	        file = files[-(i+1)]
                results.append(parse_checklink_output_file(file))
	        # pp.pprint()
	
        urls = results[0].viewkeys()
	for r in results:
                urls = urls & r.viewkeys()

        for url in sorted(urls):
                # print "processing URL: %s" % url
                persistent = results[0][url]
                # print "initial persistent = %s" % persistent
                for r in results:
                        # print "r = %s" % r
                        persistent = [x for x in persistent if x in r[url]]
                        # print "persistent = %s" % persistent
                # print "final persistent = %s" % persistent
                if len(persistent) > 0:
                        print url
                        print
                        for report in persistent:
                                print report
                                print


def paragraphs(fileobj, separator='\n'):
    if separator[-1:] != '\n': separator += '\n'
    paragraph = []
    for line in fileobj:
        if line == separator:
            if paragraph:
                yield ''.join(paragraph)
                paragraph = []
        else:
            paragraph.append(line)
    if paragraph: yield ''.join(paragraph)

listof = "\nList of broken links and other issues:\n"

def parse_checklink_output_file(file):
        "Returns a map from URLs to list of problem reports for the URL"
        result = {}
        url = None
        f = open(file, 'r')
        paragraphs = f.read().split("\n\n")
        for p in paragraphs:
                if debug: print ("<<<%s>>>" % p)
        	if re.match("Processing	", p):
                        url = p
                        continue
                if (p == "----------------------------------------"
                    # for very first record
                    or p == "\n----------------------------------------"
                    # for empty file
                    or p == ""):
                        url = None
                        continue
                if p.startswith(listof):
                        p = p[len(listof):]
                # print ("<<<%s>>>" % p)
                if not (url in result):
                        result[url] = []
                result[url].append(p)
        return result



main()
